# -*- coding: utf-8 -*-
"""Assignment1_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x0MSoWu46M4lJmMHKck5nI9vzyYW-4JP
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv('Cancer_dataset.csv')

df.head(10)

columns_to_select=[
'mean_texture',
'lymph_node_status',
]

df[columns_to_select]=df[columns_to_select].fillna(0)

print(df[columns_to_select].head(10))

X=df[columns_to_select].to_numpy()
print("Original X_train: \n",X[0:10])
print(len(X))

Y=df[['tumor_size']].fillna(0)
Y=np.array(Y)
print("Original Y_train: \n",Y[:10])
print(len(Y))
print(X.shape)
print(Y.shape)

#feature scaling
n=len(X)
indices=np.arange(n)
np.random.seed(42)
np.random.shuffle(indices)
split_indices=int(0.8*n)
X_train=X[indices[ :split_indices]]
X_test=X[indices[split_indices: ]]
Y_train=Y[indices[ :split_indices]]
Y_test=Y[indices[split_indices: ]]
X_train_copy=X.copy()
Y_train_copy=Y.copy()
X_test_copy=X.copy()
Y_test_copy=Y.copy()

print(X_train.shape)
print(Y_train.shape)

print("X_test copy: \n",X_test[:10])
print("Y_test copy: \n",Y_test[:10])
print("X_train copy: \n",X_train[:10])
print("Y_train copy: \n",Y_train[:10])

#finding cost function 1/2m(predicty-y)^2

def cost_function(X_train, Y_train, w, b):
    cost_funct=0
    m=X_train.shape[0]
    for i in range(m):
        f_wb_i=np.dot(X_train[i], w)+b
        cost_funct=cost_funct+(f_wb_i-Y_train[i])**2
    cost_function=(1/(2*m))*(cost_funct)
    return cost_function

def gradient_descent_db_dj(X_train, Y_train, w, b):
    m, n = X_train.shape
    dw = np.zeros(n, dtype=np.float64)
    '''Ensure dtype is float64 to avoid issues because the numpy newer version
    throws DeprecationWarning: due to multiplying two arrays'''
    db = 0.0

    for i in range(m):
        err = (np.dot(X_train[i], w) + b - Y_train[i]).item()
        for j in range(n):
            dw[j] += err * X_train[i, j]
        db += err

    dw /= m
    db /= m
    return dw, db

def gradient_function(X_train, Y_train, w, b, iterators, learning_rate):
    prev_loss = float('inf')
    tolerance = 1e-6
    for i in range(iterators):
        dw, db = gradient_descent_db_dj(X_train, Y_train, w, b)

        w -= learning_rate * dw
        b -= learning_rate * db
        loss = cost_function(X_train, Y_train, w, b)

        if abs(prev_loss - loss) < tolerance:  # Fixed the function call
            print(f"Converged at {i}th iteration")
            break
        prev_loss = loss

    return w, b

# Initialize parameters
initial_w = np.zeros(X_train.shape[1])
initial_b = 0.
iterators = 10000
learning_rate = 0.0001

w, b = gradient_function(X_train, Y_train, initial_w, initial_b, iterators, learning_rate)

# Final prediction
predic_val_train=np.dot(X_train, w) + b
print(f"Final prediction : {predic_val_train[:5]}")
final_val = np.dot(X_train[2], w) + b
print(f"Final prediction of row 3rd: {final_val}")

# Final prediction
final_val = np.dot(X_train[2], w) + b
print(f"Final prediction: {final_val}")

print(X_test[3])
print(Y_test[3])
# Make a prediction
final_vals = np.dot(X_test[3], w) + b

print("Final Prediction:", final_vals)

# Predictions for training and test data
y_train_pred = np.dot(X_train, w) + b
y_test_pred = np.dot(X_test, w) + b


# MSE Calculation
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# MSE for test data
test_mse = mean_squared_error(Y_test, y_test_pred)
print(f'MSE for test data: {test_mse}')

# Compute RSS & TSS
rss = np.sum((Y_test - y_test_pred) ** 2)
tss = np.sum((Y_test - np.mean(Y_test)) ** 2)

# Calculate R-squared
r_squared = 1 - (rss / tss) if tss != 0 else float('nan')

print(f"R-squared: {r_squared}")

# Number of data points (n) and number of predictors (p)
n = len(y_test_pred)
p = X_test.shape[1]
print(n,p)
# Calculate Adjusted R-squared
r_squared_adj = 1 - ((1 - r_squared) * (n - 1)) / (n - p - 1)
print(f"Adjusted R-squared: {r_squared_adj}")

"""Lets calculate BIC value for comparing this with the best one from question 3"""

def calculate_BIC(n, k, Y_train, res_ss):
    L = (-n / 2) * np.log(2 * np.pi) - (n / 2) * np.log(res_ss / n) - (res_ss / (2 * np.var(Y_train)))
    BIC = np.log(n) * k - 2 * L
    return BIC

def ols(x, y):
    # Add a column of ones to X for the intercept term (beta0)
    X_ = np.c_[np.ones(len(x)), x]
    # Compute the pseudo-inverse of X^T * X
    x_inv = np.linalg.pinv(X_.T @ X_)
    # Compute the regression coefficients (beta)
    beta = x_inv @ (X_.T) @ y
    return beta

# Fit the OLS model
beta = ols(X_train, Y_train)
print(beta)

# Make predictions
Y_pred_bic = np.c_[np.ones(X_train.shape[0]), X_train] @ beta
print(Y_pred_bic.shape[0])

# Get number of observations (n) and parameters (k)
n = Y_train.shape[0]
k = X_train.shape[1] + 1  # +1 for the intercept
print(n, k)

# Calculate residual sum of squares
ress_s = np.sum((Y_train - Y_pred_bic)**2)

# Calculate BIC
bic = calculate_BIC(n, k, Y_train, ress_s)
print(f"Value of BIC: {bic}")

#Data
metrics=['MSE','R²','Adjusted R²']
model_1=[test_mse, r_squared, r_squared_adj]


data=pd.DataFrame({
    "Metric": metrics,
    "Forward Stepwise Regression": model_1,

})

data_melted=data.melt(id_vars="Metric", var_name="Model", value_name="Value")

#Plot
plt.figure(figsize=(8,5))
plt.axhline(y=0, color='red', linestyle='--')
sns.barplot(x="Metric", y="Value", hue="Model", data=data_melted, palette=["blue", "red", "gold"])
plt.title("Showing Forward regression model metrics")
plt.ylabel("Value")
plt.xlabel("Metric")
plt.legend(loc='upper left', bbox_to_anchor=(1,1))
plt.show()

#Data
metrics=['BIC']
model_1=[bic]


data=pd.DataFrame({
    "Metric": metrics,
    "Forward Stepwise Regression": model_1,

})

data_melted=data.melt(id_vars="Metric", var_name="Model", value_name="Value")

#Plot
plt.figure(figsize=(8,5))
plt.axhline(y=0, color='red', linestyle='--')
sns.barplot(x="Metric", y="Value", hue="Model", data=data_melted, palette=[ "red"])
plt.title("Showing Forward regression model metrics")
plt.ylabel("Value")
plt.xlabel("Metric")
plt.legend(loc='upper left', bbox_to_anchor=(1,1))
plt.show()

#checking the pattern
residual=Y_train.flatten()-predic_val_train
plt.scatter(Y_train, residual, marker='x', c='b')
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('Actual target values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()
