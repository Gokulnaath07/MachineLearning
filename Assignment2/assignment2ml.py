# -*- coding: utf-8 -*-
"""Assignment2ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mr4aWsYs634FE-Ech9FKAOuwBePjPJpA
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import math

df=pd.read_csv('Cancer_dataset1.csv')
df.head(10)

df.isnull().sum()

col=['mean_radius', 'lymph_node_status','mean_texture','worst_concavity']
df[col].head(10)

df[col]=df[col].fillna(0)
df.isnull().sum()

"""1.
a. Consider the following numeric variables in the dataset: mean_radius, mean_texture,
mean_perimeter, mean_area, mean_smoothness, mean_compactness, mean_concavity and
mean_concave_points. Summarize the statistics of these variables into count, mean, standard
deviation, minimum, 25% percentile, 50% percentile, 75% percentile, and maximum.
"""

cols=['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area', 'mean_smoothness', 'mean_compactness', 'mean_concavity', 'mean_concave_points']
df_new=df[cols].to_numpy()
print(df_new[0:10])

df[cols].describe()

count = df[cols].count()
mean = df[cols].mean()
std = df[cols].std()
min = df[cols].min()
per_25 = df[cols].quantile(0.25)
per_50 = df[cols].quantile(0.50)
per_75 = df[cols].quantile(0.75)
max = df[cols].max()


datafr=pd.DataFrame({'count':count, 'mean':mean, 'std':std, 'min':min, '25%':per_25, '50%':per_50, '75%':per_75, 'max':max})
datafr

"""b. Consider the categorical variable “outcome” in the dataset. Summarize the statistics of variable
into count, unique value, top value, and frequency of top value
"""

count=df['outcome'].count()
unique=df['outcome'].unique()
top_value=df['outcome'].value_counts().index[0]
freq_top=df['outcome'].value_counts()[top_value]

print('Count:',count)
print('Unique:',unique)
print('Top value:',top_value)
print('Frequency top value:',freq_top)

"""c. Is there a way to encode outcome variable from categorical to numerical data type? If so, how
would you do that?

Yes, we can do label encoding. This type has been choosen instead of others like one hot encoding or frequency encoding or binary because there are only 2 unique values(N and R). The label encoding will map 0 and 1 to the unique values if N is given 0 then R will be 1.
"""

labels={'N':0, 'R': 1}
df['outcome']=df['outcome'].map(labels)
print(df['outcome'])

"""d. Do you think there are any redundant features present in the dataset? If so, explain how
removing it won't impact the analysis. Also, based on the experiments so far, were there any
interesting observations with respect to the variables?

"""

df_encode=df.var()
print(df_encode)
df_encode=df.corr()
print(df_encode)

plt.figure(figsize=(30, 28))
sns.heatmap(df_encode, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

df_encode=df.T.duplicated()
print(df_encode)

"""There are some size related features that are highly correlated.
They are:
1.   mean_area - mean_perimeter
2.   worst_area - worst_perimeter
3.   se_perimeter - se_area
4.   worst_radius - worst_area and worst_perimeter

From this observation we can see perimeter and area are obtained using the radius. So removing these might not affect the analysis at a greater extent. For all these the radius and other two are highly corellated in all therr mean, se and worst sizes but i have given the maximum two pairs. If the pairs are area and perimeter if the analyisis focus on the outer or boundary go with perimeter if the analysis on surface area or mass then go with the area.

What is the correlation between mean_perimeter and se_perimeter?
"""

columns=['mean_perimeter','se_perimeter']
df_cors=df[columns].corr()
plt.figure(figsize=(10,8))
sns.heatmap(df_cors, annot=True, cmap='coolwarm')
plt.title('Confusion Matrix')
plt.show()

"""If the value is between 0.5 and 0.7 they are indicated as moderaterly corellated but this doesnot affect the analysis.

2. Logistic Regression with
One Variable (20 points)
(a) Can you map the likelihood of breast cancer recurrence (outcome) based on "mean_area” feature
from the dataset?
"""

X=df['mean_area']
Y=df['outcome']
X=np.array(X)
Y=np.array(Y)
print(X.shape, Y.shape)
sp=len(X)
index=np.arange(sp)
np.random.seed(27)
np.random.shuffle(index)

split=int(0.8*sp)
X_train=X[index[ :split]]
Y_train=Y[index[: split]]
X_test=X[index[split: ]]
Y_test=Y[index[split: ]]

print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)

def sigmoid(z):
   sigm=1/(1+(np.exp(-z)))
   return sigm

print(sigmoid(2))

def costfunct(x, y, w, b):
    m = x.shape[0]
    cost = 0
    epsilon = 1e-10  # Avoid log(0) by adding a small epsilon
    for i in range(m):
        z = x[i] * w + b
        sigm = sigmoid(z)
        cost += -y[i] * np.log(sigm+epsilon) - (1 - y[i]) * np.log(1 - sigm+epsilon)
    total_cost = (1 / m) * cost
    return total_cost

m = X_train.shape
n=1
# Compute and display cost with w initialized to zeroes
initial_w = 0.
initial_b = 0.
cost = costfunct(X_train, Y_train, initial_w, initial_b)
print(cost)

def gradient_des_dj(x, y, w, b):
    """Compute the gradient of the cost function"""
    m = x.shape[0]
    dw = 0.0
    db = 0.0
    for i in range(m):
        z = sigmoid(x[i] * w + b)
        err = z - y[i]
        dw += err * x[i]  # Use x[i] because we only have one feature
        db += err
    dw /= m
    db /= m
    return dw, db

def gradient_descent(x, y, gradient_des, cost_fun, iterators, learning_rate):
    """Gradient descent optimization"""
    tolerance = 1e-6
    w = 0.01  # Start with a small random initialization
    b = 0.0
    prev_loss = float('inf')
    J_history = []

    for i in range(iterators):
        dw, db = gradient_des(x, y, w, b)

        # Update weights
        w -= learning_rate * dw
        b -= learning_rate * db

        # Calculate cost
        cost = cost_fun(x, y, w, b)
        J_history.append(cost)

        if abs(prev_loss - cost) < tolerance:
            print(f"Convergence reached at iteration {i + 1}")
            break
        prev_loss = cost

        # Print the cost every 10% of the iterations
        if i % math.ceil(iterators / 10) == 0 or i == (iterators - 1):
            print(f"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}")

    return w, b

# Train the model
w, b = gradient_descent(X_train, Y_train, gradient_des_dj, costfunct, iterators=10000, learning_rate=0.1)
print('w, b: ' + str(w) + ', ' + str(b))
# Calculate the linear combination of inputs and weights
z = X_test * w + b

predict_y = sigmoid(z)

# Convert probabilities to binary predictions (0 or 1)
binary_predictions = (predict_y >= 0.7).astype(int)

print("Predicted probabilities:", predict_y)
print("Binary predictions:", binary_predictions)

# Ensure you are calculating predictions for the entire X_test
z_test = np.dot(X_test, w) + b  # Using your model parameters
predict_y_test = sigmoid(z_test)
binary_predictions = (predict_y_test >= 0.7).astype(int)

# Ensure the sizes of Y_test and binary_predictions match before passing them to classification_report
print(len(Y_test), len(binary_predictions))  # Should match

def confusion_matrix(true_labels, predicted_labels):


    TP=np.sum((true_labels ==1) & (predicted_labels ==1))
    TN =np.sum((true_labels ==0) & (predicted_labels ==0))
    FP =np.sum((true_labels == 0) & (predicted_labels ==1))
    FN=np.sum((true_labels ==1) & (predicted_labels ==0))

    return TP, TN, FP, FN

TP, TN, FP, FN = confusion_matrix(Y_test, binary_predictions)

print(f"Confusion Matrix:\n TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}")

#Classification report

precision=TP/(TP+FP)
recall=TP/(TP+FN)
f1=2*(precision*recall)/(precision+recall)
accuracy=((TP+TN)/(TP+TN+FN+FP))
precision_z=TN/(TN+FN)
precision_o=TP/(TP+FP)
rec_z=TN/(TN+FP)
rec_o=TP/(TP+FN)
f1_z=2*(precision_z*rec_z)/(precision_z+rec_z)
f1_o=2*(precision_o*rec_o)/(precision_o+rec_o)
if (math.isnan(precision) or math.isnan(recall) or math.isnan(f1) or
    math.isnan(precision_o) or math.isnan(f1_o)):
    precision = 0.0
    recall = 0.0
    f1 = 0.0
    precision_o=0
    f1_o=0

macro_avg_precision=(precision_z+precision_o)/2
macro_avg_recall=(rec_z+rec_o)/2
macro_avg_f1=(f1_z+f1_o)/2
weighted_avg_precision=(precision_z*TN + precision_o*FN)/(FN+TN)
weighted_avg_recall=(rec_z*TN + rec_o*FN)/(FN+TN)
weighted_avg_f1=(f1_z*TN + f1_o*FN)/(FN+TN)

class_rep={'Metric': ['Precision_1', 'Recall_1', 'F1-Score_1', 'Precision_0', 'Recall_0', 'F1-Score_0' ,'Accuracy', 'Macro Average Precision', 'Macro Average Recall', 'Macro Average F1 Score', 'Weighted Average precision', 'Weighted average Recall', 'Weighted Average F1']
                  , 'Value': [round(x,2) for x in [precision, recall, f1, precision_z, rec_z, f1_z, accuracy, macro_avg_precision, macro_avg_recall, macro_avg_f1, weighted_avg_precision, weighted_avg_recall, weighted_avg_f1]]}

class_rep_metric=pd.DataFrame(class_rep)
print(class_rep_metric)

metrics=['Precision', 'Recall', 'F1-Score']
model_1=[precision, recall, f1]
model_2=[precision_z, rec_z, f1_z]

data=pd.DataFrame({
    "Metrics": metrics,
    "Class 1(R)": model_1,
    "Class 0(N)": model_2

})
data_melted=data.melt(id_vars="Metrics", var_name="Class", value_name="Values" )
sns.set(style="whitegrid")
sns.barplot(x="Metrics", y="Values", hue="Class", data=data_melted, palette=["blue", "red"])

plt.title("Class 0(N) vs Class 1(R)")
plt.ylabel("Value")
plt.xlabel("Metric")
plt.show()

X = df['mean_area'].values
Y = df['outcome'].values

print(w, b)
X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)


Y_prob = sigmoid(np.dot(X_range, w) + b)

# Plotting the actual data points and sigmoid curve
plt.scatter(X, Y, color='red', label='Actual Data')
plt.plot(X_range, Y_prob, color='blue', label='Sigmoid Curve')
plt.xlabel('Mean Area')
plt.ylabel('Probability of Recurrence')
plt.legend()
plt.title('Logistic Regression: Cancer Recurrence vs. Mean Area')
plt.show()

"""3.	Logistic Regression with Multiple Variables (50 points)   
   
a)	Design a Logistic Regression model to predict breast cancer recurrence (outcome) using the following 12 variables from the dataset as input features:   
   
Features:  mean_radius, mean_texture, mean_perimeter, mean_area, mean_smoothness, mean_compactness, mean_concavity, mean_concave_points, mean_fractal_dimension, se_perimeter, se_texture, se_area   

"""

cols=['mean_radius', 'mean_texture',
      'mean_perimeter', 'mean_area',
      'mean_smoothness', 'mean_compactness',
      'mean_concavity', 'mean_concave_points',
      'mean_fractal_dimension', 'se_perimeter',
      'se_texture', 'se_area']

X=df[cols]
Y=df['outcome']
X.shape, Y.shape

X=X.to_numpy()
Y=Y.to_numpy()
print(X.shape, Y.shape)
print(X[0:10])

leng=len(X)
index=np.arange(leng)
split=int(0.75*leng)
X_train=X[index[:split]]
Y_train=Y[index[:split]]
X_test=X[index[split:]]
Y_test=Y[index[split:]]

print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def costfunct(x, y, w, b):
    m = x.shape[0]
    cost = 0
    epsi = 1e-10  # Prevent log(0)
    for i in range(m):
        z = np.dot(x[i], w) + b
        sigm = sigmoid(z)
        cost += -y[i] * np.log(sigm + epsi) - (1 - y[i]) * np.log(1 - sigm + epsi)
    return cost / m

def gradient_dj(x, y, w, b):
    m, n = x.shape
    dw = np.zeros(n)
    db = 0.0
    for i in range(m):
        z = np.dot(x[i], w) + b
        sigm = sigmoid(z)
        error = sigm - y[i]  # Correct error calculation
        for j in range(n):
            dw[j] += error * x[i, j]
        db += error
    dw /= m
    db /= m
    return dw, db

def gradient_descent(x, y, gradient_dj, costfunct, iterators, learning_rate):
    tolerance = 1e-6
    prev_loss = float('inf')
    n = x.shape[1]
    w = np.zeros(n)
    b = 0.0
    loss_history=[]
    for i in range(iterators):
        dw, db = gradient_dj(x, y, w, b)
        w -= learning_rate * dw
        b -= learning_rate * db

        loss = costfunct(x, y, w, b)
        loss_history.append(loss)
        if abs(prev_loss - loss) < tolerance:
            print(f"Convergence at the {i+1}th iteration")
            break
        prev_loss = loss

    #track loss history
    plt.plot(loss_history)
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.title('Loss History')
    plt.show()

    return w, b  # Move return outside the loop

def evaluate_accuracy(X, Y, w, b):
    z = np.dot(X, w) + b
    predictions = (sigmoid(z) >= 0.65).astype(int)  # Convert probabilities to binary (0 or 1)
    return np.mean(predictions == Y)  # Calculate accuracy

w, b = gradient_descent(X_train, Y_train, gradient_dj, costfunct, iterators=10000, learning_rate=0.001)
z = np.dot(X_test, w) + b
predict_y = sigmoid(z)
binary_predictions = (predict_y >= 0.65).astype(int)

print(binary_predictions)
print("Training set accuracy:",evaluate_accuracy(X_train, Y_train, w, b))
print("Test Accuracy for Logisic regression with 12 features: ", evaluate_accuracy(X_test, Y_test, w, b))

z_test = np.dot(X_test, w) + b  # Using your model parameters
predict_y_test = sigmoid(z_test)
binary_predictions = (predict_y_test >= 0.65).astype(int)

print(len(Y_test), len(binary_predictions))  # Should match

def confusion_matrix(true_labels, predicted_labels):
    TP = np.sum((true_labels == 1) & (predicted_labels == 1))
    TN = np.sum((true_labels == 0) & (predicted_labels == 0))
    FP = np.sum((true_labels == 0) & (predicted_labels == 1))
    FN = np.sum((true_labels == 1) & (predicted_labels == 0))
    return TP, TN, FP, FN

TP, TN, FP, FN = confusion_matrix(Y_test, binary_predictions)
print(f"Confusion Matrix:\n TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}")

# accuracy formula
accuracy = (TP + TN) / (TP + TN + FN + FP)

# Prevent division by zero
precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

precision_z = TN / (TN + FN) if (TN + FN) > 0 else 0.0
precision_o = TP / (TP + FP) if (TP + FP) > 0 else 0.0
rec_z = TN / (TN + FP) if (TN + FP) > 0 else 0.0
rec_o = TP / (TP + FN) if (TP + FN) > 0 else 0.0
f1_z = 2 * (precision_z * rec_z) / (precision_z + rec_z) if (precision_z + rec_z) > 0 else 0.0
f1_o = 2 * (precision_o * rec_o) / (precision_o + rec_o) if (precision_o + rec_o) > 0 else 0.0
# macro averages
macro_avg_precision = (precision_z + precision_o) / 2
macro_avg_recall = (rec_z + rec_o) / 2
macro_avg_f1 = (f1_z + f1_o) / 2

# weighted averages
total_samples = TP + TN + FP + FN
weighted_avg_precision = (precision_z * (TN + FP) + precision_o * (TP + FN)) / total_samples
weighted_avg_recall = (rec_z * (TN + FP) + rec_o * (TP + FN)) / total_samples
weighted_avg_f1 = (f1_z * (TN + FP) + f1_o * (TP + FN)) / total_samples

# Create classification report dataframe
class_rep = {
    'Metric': [
        'Precision_1', 'Recall_1', 'F1-Score_1',
        'Precision_0', 'Recall_0', 'F1-Score_0',
        'Accuracy', 'Macro Average Precision', 'Macro Average Recall',
        'Macro Average F1 Score', 'Weighted Average Precision',
        'Weighted Average Recall', 'Weighted Average F1'
    ],
    'Value': [round(x, 2) for x in [
        precision, recall, f1, precision_z, rec_z, f1_z, accuracy,
        macro_avg_precision, macro_avg_recall, macro_avg_f1,
        weighted_avg_precision, weighted_avg_recall, weighted_avg_f1
    ]]
}

class_rep_metric = pd.DataFrame(class_rep)
print("Classification Report for Logistic Regression Before Regularization and Feature Scaling")
print(class_rep_metric)

print(X_train.shape[1])

"""b)	Design a Logistic Regression model to predict breast cancer recurrence (outcome) using forward selection to select the most significant variables in the dataset as input features. Which subset of features gave you the best performance? What are your thoughts on these features getting selected? (Use 12 features from 3a as Input features)   

"""

def forward_step():
  selected_columns = []  # Start with an empty list of selected features
  remaining_features = list(range(X_train.shape[1]))  # List of all feature indices
  current_acc=-float('inf')
  # n=X_train.shape[1]
  # Stepwise regression
  for i in range(X_train.shape[1]):
      best_acc = current_acc
      best_feature = None
      best_w, best_b = None, None

      # Try adding each remaining feature
      for feature in remaining_features:
          new_features = selected_columns + [feature]  # Add current feature to selected features
          print(new_features)
          model = X_train[:, new_features]  # Select the columns for the current model



          w,b = gradient_descent(model, Y_train, gradient_dj, costfunct, iterators=10000, learning_rate=0.001)

          X_subset = X_train[:, new_features]
          model_test=X_test[:, new_features]
          acc = evaluate_accuracy(X_subset, Y_train, w, b)
          acc_test=evaluate_accuracy(model_test, Y_test, w, b)

          z=np.dot(model_test, w)+b
          predict_y=sigmoid(z)
          binary_predictions=(predict_y>=0.65).astype(int)
          conf_mat=confusion_matrix(Y_test, binary_predictions)
          print(f"Confusion Matrix:\n TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}")

          # Keep the best feature based on ACC
          if acc > best_acc:
              best_acc = acc
              best_test_acc=acc_test
              best_feature = feature
              best_model = model
              best_w, best_b = w,b

      # If ACC improves, add the feature and update remaining features
      if best_feature is not None:
          selected_columns.append(best_feature)  # Add the best feature to selected
          remaining_features.remove(best_feature)  # Remove the best feature from remaining
          current_acc = best_acc
          print(f"Iteration {i+1}: Added feature {best_feature}, ACC: {best_acc}")
          print(f"Best test Accuaracy: {best_test_acc}")
      else:
          print(f"ACC value obtained: {acc} which is smaller than the best Acc obtained before so the loop stoped")
          break


  return selected_columns, best_acc, best_test_acc, TP, TN, FP, FN

selected_columns, bestAcc, bestTestAcc,TP, TN, FP, FN = forward_step()
print(len(selected_columns))
selected_columns_new=[]
# Final selected features
for i in range(len(selected_columns)):
  Best_features=cols[selected_columns[i]]
  selected_columns_new.append(Best_features)
  print(Best_features)
print(f"Best training accc: {bestAcc}")
print(f"Test accuracy using forward stepwise regression: {bestTestAcc}")

def confusion_matrix(true_labels, predicted_labels):
    TP = np.sum((true_labels == 1) & (predicted_labels == 1))
    TN = np.sum((true_labels == 0) & (predicted_labels == 0))
    FP = np.sum((true_labels == 0) & (predicted_labels == 1))
    FN = np.sum((true_labels == 1) & (predicted_labels == 0))
    return TP, TN, FP, FN

TP, TN, FP, FN = confusion_matrix(Y_test, binary_predictions)

# accuracy formula
accuracy = (TP + TN) / (TP + TN + FN + FP)

# Prevent division by zero
precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

precision_z = TN / (TN + FN) if (TN + FN) > 0 else 0.0
precision_o = TP / (TP + FP) if (TP + FP) > 0 else 0.0
rec_z = TN / (TN + FP) if (TN + FP) > 0 else 0.0
rec_o = TP / (TP + FN) if (TP + FN) > 0 else 0.0
f1_z = 2 * (precision_z * rec_z) / (precision_z + rec_z) if (precision_z + rec_z) > 0 else 0.0
f1_o = 2 * (precision_o * rec_o) / (precision_o + rec_o) if (precision_o + rec_o) > 0 else 0.0

# macro averages
macro_avg_precision = (precision_z + precision_o) / 2
macro_avg_recall = (rec_z + rec_o) / 2
macro_avg_f1 = (f1_z + f1_o) / 2

# weighted averages
total_samples = TP + TN + FP + FN
weighted_avg_precision = (precision_z * (TN + FP) + precision_o * (TP + FN)) / total_samples
weighted_avg_recall = (rec_z * (TN + FP) + rec_o * (TP + FN)) / total_samples
weighted_avg_f1 = (f1_z * (TN + FP) + f1_o * (TP + FN)) / total_samples

# Create classification report dataframe
class_rep = {
    'Metric': [
        'Precision_1', 'Recall_1', 'F1-Score_1',
        'Precision_0', 'Recall_0', 'F1-Score_0',
        'Accuracy', 'Macro Average Precision', 'Macro Average Recall',
        'Macro Average F1 Score', 'Weighted Average Precision',
        'Weighted Average Recall', 'Weighted Average F1'
    ],
    'Value': [round(x, 2) for x in [
        precision, recall, f1, precision_z, rec_z, f1_z, accuracy,
        macro_avg_precision, macro_avg_recall, macro_avg_f1,
        weighted_avg_precision, weighted_avg_recall, weighted_avg_f1
    ]]
}

class_rep_metric = pd.DataFrame(class_rep)
print("Classification Report for Logistic Regression Before Regularization and Feature Scaling")
print(class_rep_metric)

print(len(selected_columns))

"""3.c.

"""

cols=['mean_radius', 'mean_texture']

X=df[cols]
Y=df['outcome']
X.shape, Y.shape

X=X.to_numpy()
Y=Y.to_numpy()
print(X.shape, Y.shape)
print(X[0:10])

leng=len(X)
index=np.arange(leng)
split=int(0.75*leng)
X_train=X[index[:split]]
X_test=X[index[split:]]

Y_train=Y[index[:split]]
Y_test=Y[index[split:]]

print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)
print("Xtrain:",X_train)
print("Ytrain",Y_train)
print("Xtest",X_test)
print("Ytest",Y_test)

def costfunct(x, y, w, b):
    m = x.shape[0]
    cost = 0
    epsi = 1e-10  # Prevent log(0)
    for i in range(m):
        z = np.dot(x[i], w) + b
        sigm = sigmoid(z)
        cost += -y[i] * np.log(sigm + epsi) - (1 - y[i]) * np.log(1 - sigm + epsi)
    return cost / m

def gradient_dj(x, y, w, b):
    m, n = x.shape
    dw = np.zeros(n)
    db = 0.0
    for i in range(m):
        z = np.dot(x[i], w) + b
        sigm = sigmoid(z)
        error = sigm - y[i]  # Correct error calculation
        for j in range(n):
            dw[j] += error * x[i, j]
        db += error
    dw /= m
    db /= m
    return dw, db
def gradient_descent(x, y, gradient_dj, costfunct, iterators, learning_rate):
    tolerance = 1e-6
    prev_loss = float('inf')
    n = x.shape[1]
    w = np.zeros(n)
    b = 0.0
    loss_history=[]
    for i in range(iterators):
        dw, db = gradient_dj(x, y, w, b)
        w -= learning_rate * dw
        b -= learning_rate * db

        loss = costfunct(x, y, w, b)
        loss_history.append(loss)
        if abs(prev_loss - loss) < tolerance:
            print(f"Convergence at the {i+1}th iteration")
            break
        prev_loss = loss

    #track loss history
    plt.plot(loss_history)
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.title('Loss History')
    plt.show()

    return w, b  # Move return outside the loop
def evaluate_accuracy(X, Y, w, b):
    z = np.dot(X, w) + b
    predictions = (sigmoid(z) >= 0.65).astype(int)  # Convert probabilities to binary (0 or 1)
    return np.mean(predictions == Y)  # Calculate accuracy
w, b = gradient_descent(X_train, Y_train, gradient_dj, costfunct, iterators=10000, learning_rate=0.001)
z = np.dot(X_test, w) + b
predict_y = sigmoid(z)
binary_predictions = (predict_y >= 0.65).astype(int)

print(binary_predictions)
print("Training set accuracy:",evaluate_accuracy(X_train, Y_train, w, b))
print("Test Accuracy for Logisic regression with 12 features: ", evaluate_accuracy(X_test, Y_test, w, b))

"""Both gives same accuracy both are good in terms of accuracy that is the metric used mainly for comaparison

4.	Experimenting with regularization and Cost function. (40 points)   
   
a)	Regularization and Feature Scaling: (20 points)   
I.	For the best performing model in Q.3 (Model from 3c), does regularization improve the performance? II. Does Feature Scaling improve the performance for the model in Q 3c?

Selected model is Logistic regression model
"""

print(selected_columns_new)

"""1. Using regularization"""

cols=['mean_radius', 'mean_texture',
      'mean_perimeter', 'mean_area',
      'mean_smoothness', 'mean_compactness',
      'mean_concavity', 'mean_concave_points',
      'mean_fractal_dimension', 'se_perimeter',
      'se_texture', 'se_area']

X=df[cols]
Y=df['outcome']

X.shape, Y.shape

X=X.to_numpy()
Y=Y.to_numpy()
leng=len(X)
index=np.arange(leng)
split=int(0.75*leng)
X_train=X[index[:split]]
X_test=X[index[split:]]

# for i in range(X_train.shape[1]):
#   mean=np.mean(X_train[:, i])
#   std=np.std(X_train[:, i])
#   X_train[:, i]= (X_train[:, i]-mean)/std
#   X_test[:, i]= (X_test[:, i]-mean)/std


Y_train=Y[index[:split]]
Y_test=Y[index[split:]]

print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)
print("Xtrain:",X_train)
print("Ytrain",Y_train)
print("Xtest",X_test)
print("Ytest",Y_test)

def costfunct_reg(x, y, w, b, lambda_val):
    m = x.shape[0]
    cost = 0
    epsi = 1e-10  # Prevent log(0)
    for i in range(m):
        z = np.dot(x[i], w) + b
        sigm = sigmoid(z)
        cost += -y[i] * np.log(sigm + epsi) - (1 - y[i]) * np.log(1 - sigm + epsi)
    reg_term=(lambda_val/(2*m))*np.sum(w**2)
    return cost / m + reg_term

def gradient_dj_reg(x, y, w, b, lambda_val):
    m, n = x.shape
    dw = np.zeros(n)
    db = 0.0
    for i in range(m):
        z = np.dot(x[i], w) + b
        sigm = sigmoid(z)
        error = sigm - y[i]  # Correct error calculation
        for j in range(n):
            dw[j] += error * x[i, j]
        db += error
    dw /= m
    dw += (lambda_val/m)*w
    db /= m
    return dw, db

def gradient_descent_reg(x, y, gradient_dj_reg, costfunct_reg, iterators, learning_rate, lambda_val):
    tolerance = 1e-6
    prev_loss = float('inf')
    n = x.shape[1]
    w = np.zeros(n)
    b = 0.0
    loss_history=[]
    for i in range(iterators):
        dw, db = gradient_dj_reg(x, y, w, b, lambda_val)
        w -= learning_rate * dw
        b -= learning_rate * db

        loss = costfunct_reg(x, y, w, b, lambda_val)
        loss_history.append(loss)
        if abs(prev_loss - loss) < tolerance:
            print(f"Convergence at the {i+1}th iteration")
            break
        prev_loss = loss

    #track loss history
    plt.plot(loss_history)
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.title('Loss History')
    plt.show()

    return w, b  # Move return outside the loop

w, b = gradient_descent_reg(X_train, Y_train, gradient_dj_reg, costfunct_reg, iterators=10000, learning_rate=0.001, lambda_val=0.1)
z = np.dot(X_test, w) + b
predict_y = sigmoid(z)
binary_predictions = (predict_y >= 0.65).astype(int)

print(binary_predictions)
print("Test Accuracy: ", evaluate_accuracy(X_test, Y_test, w, b))

def confusion_matrix(true_labels, predicted_labels):
    TP = np.sum((true_labels == 1) & (predicted_labels == 1))
    TN = np.sum((true_labels == 0) & (predicted_labels == 0))
    FP = np.sum((true_labels == 0) & (predicted_labels == 1))
    FN = np.sum((true_labels == 1) & (predicted_labels == 0))
    return TP, TN, FP, FN

TP, TN, FP, FN = confusion_matrix(Y_test, binary_predictions)
print(f"Confusion Matrix:\n TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}")

accuracy = (TP + TN) / (TP + TN + FN + FP)

# Prevent division by zero
precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

precision_z = TN / (TN + FN) if (TN + FN) > 0 else 0.0
precision_o = TP / (TP + FP) if (TP + FP) > 0 else 0.0
rec_z = TN / (TN + FP) if (TN + FP) > 0 else 0.0
rec_o = TP / (TP + FN) if (TP + FN) > 0 else 0.0
f1_z = 2 * (precision_z * rec_z) / (precision_z + rec_z) if (precision_z + rec_z) > 0 else 0.0
f1_o = 2 * (precision_o * rec_o) / (precision_o + rec_o) if (precision_o + rec_o) > 0 else 0.0

macro_avg_precision = (precision_z + precision_o) / 2
macro_avg_recall = (rec_z + rec_o) / 2
macro_avg_f1 = (f1_z + f1_o) / 2

total_samples = TP + TN + FP + FN
weighted_avg_precision = (precision_z * (TN + FP) + precision_o * (TP + FN)) / total_samples
weighted_avg_recall = (rec_z * (TN + FP) + rec_o * (TP + FN)) / total_samples
weighted_avg_f1 = (f1_z * (TN + FP) + f1_o * (TP + FN)) / total_samples

# Create classification report dataframe
class_rep = {
    'Metric': [
        'Precision_1', 'Recall_1', 'F1-Score_1',
        'Precision_0', 'Recall_0', 'F1-Score_0',
        'Accuracy', 'Macro Average Precision', 'Macro Average Recall',
        'Macro Average F1 Score', 'Weighted Average Precision',
        'Weighted Average Recall', 'Weighted Average F1'
    ],
    'Value': [round(x, 2) for x in [
        precision, recall, f1, precision_z, rec_z, f1_z, accuracy,
        macro_avg_precision, macro_avg_recall, macro_avg_f1,
        weighted_avg_precision, weighted_avg_recall, weighted_avg_f1
    ]]
}

class_rep_metric = pd.DataFrame(class_rep)
print("Classification Report for Logistic Regression after regularization")
print(class_rep_metric)

"""11. Using feature scaling"""

X=df[cols]
Y=df['outcome']

X.shape, Y.shape

X=X.to_numpy()
Y=Y.to_numpy()
leng=len(X)
index=np.arange(leng)
split=int(0.75*leng)
X_train=X[index[:split]]
X_test=X[index[split:]]

for i in range(X_train.shape[1]):
  mean=np.mean(X_train[:, i])
  std=np.std(X_train[:, i])
  X_train[:, i]= (X_train[:, i]-mean)/std
  X_test[:, i]= (X_test[:, i]-mean)/std


Y_train=Y[index[:split]]
Y_test=Y[index[split:]]

X_train.shape, Y_train.shape
X_test.shape, Y_test.shape
print("Xtrain:",X_train)
print("Ytrain",Y_train)
print("Xtest",X_test)
print("Ytest",Y_test)

w, b = gradient_descent(X_train, Y_train, gradient_dj, costfunct, iterators=10000, learning_rate=0.1)
z = np.dot(X_test, w) + b
predict_y = sigmoid(z)
binary_predictions = (predict_y >= 0.65).astype(int)

print(binary_predictions)
print("Test Accuracy: ", evaluate_accuracy(X_test, Y_test, w, b))

def confusion_matrix(true_labels, predicted_labels):
    TP = np.sum((true_labels == 1) & (predicted_labels == 1))
    TN = np.sum((true_labels == 0) & (predicted_labels == 0))
    FP = np.sum((true_labels == 0) & (predicted_labels == 1))
    FN = np.sum((true_labels == 1) & (predicted_labels == 0))
    return TP, TN, FP, FN

TP, TN, FP, FN = confusion_matrix(Y_test, binary_predictions)
print(f"Confusion Matrix:\n TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}")
accuracy = (TP + TN) / (TP + TN + FN + FP)

# Prevent division by zero
precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

precision_z = TN / (TN + FN) if (TN + FN) > 0 else 0.0
precision_o = TP / (TP + FP) if (TP + FP) > 0 else 0.0
rec_z = TN / (TN + FP) if (TN + FP) > 0 else 0.0
rec_o = TP / (TP + FN) if (TP + FN) > 0 else 0.0
f1_z = 2 * (precision_z * rec_z) / (precision_z + rec_z) if (precision_z + rec_z) > 0 else 0.0
f1_o = 2 * (precision_o * rec_o) / (precision_o + rec_o) if (precision_o + rec_o) > 0 else 0.0

macro_avg_precision = (precision_z + precision_o) / 2
macro_avg_recall = (rec_z + rec_o) / 2
macro_avg_f1 = (f1_z + f1_o) / 2

total_samples = TP + TN + FP + FN
weighted_avg_precision = (precision_z * (TN + FP) + precision_o * (TP + FN)) / total_samples
weighted_avg_recall = (rec_z * (TN + FP) + rec_o * (TP + FN)) / total_samples
weighted_avg_f1 = (f1_z * (TN + FP) + f1_o * (TP + FN)) / total_samples

# Create classification report dataframe
class_rep = {
    'Metric': [
        'Precision_1', 'Recall_1', 'F1-Score_1',
        'Precision_0', 'Recall_0', 'F1-Score_0',
        'Accuracy', 'Macro Average Precision', 'Macro Average Recall',
        'Macro Average F1 Score', 'Weighted Average Precision',
        'Weighted Average Recall', 'Weighted Average F1'
    ],
    'Value': [round(x, 2) for x in [
        precision, recall, f1, precision_z, rec_z, f1_z, accuracy,
        macro_avg_precision, macro_avg_recall, macro_avg_f1,
        weighted_avg_precision, weighted_avg_recall, weighted_avg_f1
    ]]
}

class_rep_metric = pd.DataFrame(class_rep)
print("Classification Report for Logistic Regression after Feature Scaling")
print(class_rep_metric)

"""Using both Methods"""

X=df[cols]
Y=df['outcome']

X.shape, Y.shape

X=X.to_numpy()
Y=Y.to_numpy()
leng=len(X)
index=np.arange(leng)
split=int(0.75*leng)
X_train=X[index[:split]]
X_test=X[index[split:]]

for i in range(X_train.shape[1]):
  mean=np.mean(X_train[:, i])
  std=np.std(X_train[:, i])
  X_train[:, i]= (X_train[:, i]-mean)/std
  X_test[:, i]= (X_test[:, i]-mean)/std


Y_train=Y[index[:split]]
Y_test=Y[index[split:]]

print("Training shape: ", X_train.shape, Y_train.shape)
print(f"Test Shape: {X_test.shape} ,{Y_test.shape}")
print("Xtrain:",X_train)
print("Ytrain",Y_train)
print("Xtest",X_test)
print("Ytest",Y_test)

w, b = gradient_descent_reg(X_train, Y_train, gradient_dj_reg, costfunct_reg, iterators=10000, learning_rate=0.001, lambda_val=0.1)
z = np.dot(X_test, w) + b
predict_y = sigmoid(z)
binary_predictions = (predict_y >= 0.65).astype(int)

print(binary_predictions)
print("Test Accuracy: ", evaluate_accuracy(X_test, Y_test, w, b))

def confusion_matrix(true_labels, predicted_labels):
    TP = np.sum((true_labels == 1) & (predicted_labels == 1))
    TN = np.sum((true_labels == 0) & (predicted_labels == 0))
    FP = np.sum((true_labels == 0) & (predicted_labels == 1))
    FN = np.sum((true_labels == 1) & (predicted_labels == 0))
    return TP, TN, FP, FN

TP, TN, FP, FN = confusion_matrix(Y_test, binary_predictions)
print(f"Confusion Matrix:\n TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}")
accuracy = (TP + TN) / (TP + TN + FN + FP)

# Prevent division by zero
precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

precision_z = TN / (TN + FN) if (TN + FN) > 0 else 0.0
precision_o = TP / (TP + FP) if (TP + FP) > 0 else 0.0
rec_z = TN / (TN + FP) if (TN + FP) > 0 else 0.0
rec_o = TP / (TP + FN) if (TP + FN) > 0 else 0.0
f1_z = 2 * (precision_z * rec_z) / (precision_z + rec_z) if (precision_z + rec_z) > 0 else 0.0
f1_o = 2 * (precision_o * rec_o) / (precision_o + rec_o) if (precision_o + rec_o) > 0 else 0.0

macro_avg_precision = (precision_z + precision_o) / 2
macro_avg_recall = (rec_z + rec_o) / 2
macro_avg_f1 = (f1_z + f1_o) / 2

total_samples = TP + TN + FP + FN
weighted_avg_precision = (precision_z * (TN + FP) + precision_o * (TP + FN)) / total_samples
weighted_avg_recall = (rec_z * (TN + FP) + rec_o * (TP + FN)) / total_samples
weighted_avg_f1 = (f1_z * (TN + FP) + f1_o * (TP + FN)) / total_samples

class_rep = {
    'Metric': [
        'Precision_1', 'Recall_1', 'F1-Score_1',
        'Precision_0', 'Recall_0', 'F1-Score_0',
        'Accuracy', 'Macro Average Precision', 'Macro Average Recall',
        'Macro Average F1 Score', 'Weighted Average Precision',
        'Weighted Average Recall', 'Weighted Average F1'
    ],
    'Value': [round(x, 2) for x in [
        precision, recall, f1, precision_z, rec_z, f1_z, accuracy,
        macro_avg_precision, macro_avg_recall, macro_avg_f1,
        weighted_avg_precision, weighted_avg_recall, weighted_avg_f1
    ]]
}

class_rep_metric = pd.DataFrame(class_rep)
print("Classification Report for Logistic Regression after Feature Scaling")
print(class_rep_metric)

# def forward_step():
#   selected_columns = []  # Start with an empty list of selected features
#   remaining_features = list(range(X_train.shape[1]))  # List of all feature indices
#   current_acc=-float('inf')
#   # n=X_train.shape[1]
#   # Stepwise regression
#   for i in range(X_train.shape[1]):
#       best_acc = current_acc
#       best_feature = None
#       best_w, best_b = None, None

#       # Try adding each remaining feature
#       for feature in remaining_features:
#           new_features = selected_columns + [feature]  # Add current feature to selected features
#           print(new_features)
#           model = X_train[:, new_features]  # Select the columns for the current model



#           w,b = gradient_descent(model, Y_train, gradient_dj, costfunct, iterators=10000, learning_rate=0.001, lambda_val=0.1)

#           X_subset = X_train[:, new_features]
#           model_test=X_test[:, new_features]
#           acc = evaluate_accuracy(X_subset, Y_train, w, b)
#           acc_test=evaluate_accuracy(model_test, Y_test, w, b)

#           z=np.dot(model_test, w)+b
#           predict_y=sigmoid(z)
#           binary_predictions=(predict_y>=0.65).astype(int)
#           conf_mat=confusion_matrix(Y_test, binary_predictions)
#           print(f"Confusion Matrix:\n TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}")

#           # Keep the best feature based on ACC
#           if acc > best_acc:
#               best_acc = acc
#               best_test_acc=acc_test
#               best_feature = feature
#               best_model = model
#               best_w, best_b = w,b

#       # If ACC improves, add the feature and update remaining features
#       if best_feature is not None:
#           selected_columns.append(best_feature)  # Add the best feature to selected
#           remaining_features.remove(best_feature)  # Remove the best feature from remaining
#           current_acc = best_acc
#           print(f"Iteration {i+1}: Added feature {best_feature}, ACC: {best_acc}")
#           print(f"Best test Accuaracy: {best_test_acc}")
#       else:
#           print(f"ACC value obtained: {acc} which is smaller than the best Acc obtained before so the loop stoped")
#           break


#   return selected_columns, best_acc, best_test_acc, TP, TN, FP, FN

# selected_columns, bestAcc, bestTestAcc,TP, TN, FP, FN = forward_step()
# print(len(selected_columns))
# selected_columns_new=[]
# # Final selected features
# for i in range(len(selected_columns)):
#   Best_features=cols[selected_columns[i]]
#   selected_columns_new.append(Best_features)
#   print(Best_features)
# print(f"Best accc: {bestAcc}")
# print(f"Best Test acc: {bestTestAcc}")

# def confusion_matrix(true_labels, predicted_labels):
#     TP = np.sum((true_labels == 1) & (predicted_labels == 1))
#     TN = np.sum((true_labels == 0) & (predicted_labels == 0))
#     FP = np.sum((true_labels == 0) & (predicted_labels == 1))
#     FN = np.sum((true_labels == 1) & (predicted_labels == 0))
#     return TP, TN, FP, FN

# TP, TN, FP, FN = confusion_matrix(Y_test, binary_predictions)
# print(f"Confusion Matrix:\n TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}")
# # Fix accuracy formula
# accuracy = (TP + TN) / (TP + TN + FN + FP)

# # Prevent division by zero
# precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
# recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
# f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

# precision_z = TN / (TN + FN) if (TN + FN) > 0 else 0.0
# precision_o = TP / (TP + FP) if (TP + FP) > 0 else 0.0
# rec_z = TN / (TN + FP) if (TN + FP) > 0 else 0.0
# rec_o = TP / (TP + FN) if (TP + FN) > 0 else 0.0
# f1_z = 2 * (precision_z * rec_z) / (precision_z + rec_z) if (precision_z + rec_z) > 0 else 0.0
# f1_o = 2 * (precision_o * rec_o) / (precision_o + rec_o) if (precision_o + rec_o) > 0 else 0.0

# # Fix macro averages
# macro_avg_precision = (precision_z + precision_o) / 2
# macro_avg_recall = (rec_z + rec_o) / 2
# macro_avg_f1 = (f1_z + f1_o) / 2

# # Fix weighted averages
# total_samples = TP + TN + FP + FN
# weighted_avg_precision = (precision_z * (TN + FP) + precision_o * (TP + FN)) / total_samples
# weighted_avg_recall = (rec_z * (TN + FP) + rec_o * (TP + FN)) / total_samples
# weighted_avg_f1 = (f1_z * (TN + FP) + f1_o * (TP + FN)) / total_samples

# # Create classification report dataframe
# class_rep = {
#     'Metric': [
#         'Precision_1', 'Recall_1', 'F1-Score_1',
#         'Precision_0', 'Recall_0', 'F1-Score_0',
#         'Accuracy', 'Macro Average Precision', 'Macro Average Recall',
#         'Macro Average F1 Score', 'Weighted Average Precision',
#         'Weighted Average Recall', 'Weighted Average F1'
#     ],
#     'Value': [round(x, 2) for x in [
#         precision, recall, f1, precision_z, rec_z, f1_z, accuracy,
#         macro_avg_precision, macro_avg_recall, macro_avg_f1,
#         weighted_avg_precision, weighted_avg_recall, weighted_avg_f1
#     ]]
# }

# class_rep_metric = pd.DataFrame(class_rep)
# print("Classification Report for Logistic Regression Before Regularization and Feature Scaling")
# print(class_rep_metric)

"""Using different cost function"""

X=df[cols]
Y=df['outcome']
X.shape, Y.shape

X=X.to_numpy()
Y=Y.to_numpy()
print(X.shape, Y.shape)
print(X[0:10])
leng=len(X)
index=np.arange(leng)
split=int(0.75*leng)
for i in range(X_train.shape[1]):
  mean=np.mean(X_train[:, i])
  std=np.std(X_train[:, i])
  X_train[:, i]= (X_train[:, i]-mean)/std
  X_test[:, i]= (X_test[:, i]-mean)/std

X_test=X[index[split:]]
Y_test=Y[index[split:]]

print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)

def costfunct(x, y, w, b, lambda_val):
    m = x.shape[0]
    cost = 0
    epsi = 1e-10  # Prevent log(0)
    for i in range(m):
        z = np.dot(x[i], w) + b
        sigm = sigmoid(z)
        cost += (sigm-y[i])**2
        reg_term=(lambda_val/(2*m))*np.sum(w**2)
    return cost / (2*m)+reg_term

def gradient_dj(x, y, w, b, lambda_val):
    m, n = x.shape
    dw = np.zeros(n)
    db = 0.0
    for i in range(m):
        z = np.dot(x[i], w) + b
        sigm = sigmoid(z)
        error = sigm - y[i]
        for j in range(n):
            dw[j] += error * x[i, j]
        db += error
    dw /= m +(lambda_val/m)*w
    db /= m
    return dw, db

def gradient_descent(x, y, gradient_dj, costfunct, iterators, learning_rate, lambda_val):
    tolerance = 1e-6
    prev_loss = float('inf')
    n = x.shape[1]
    w = np.zeros(n)
    b = 0.0
    loss_history=[]
    for i in range(iterators):
        dw, db = gradient_dj(x, y, w, b, lambda_val)
        w -= learning_rate * dw
        b -= learning_rate * db

        loss = costfunct(x, y, w, b, lambda_val)
        loss_history.append(loss)
        if abs(prev_loss - loss) < tolerance:
            print(f"Convergence at the {i+1}th iteration")
            break
        prev_loss = loss

    #track loss history
    plt.plot(loss_history)
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.title('Loss History')
    plt.show()

    return w, b

w, b = gradient_descent(X_train, Y_train, gradient_dj, costfunct, iterators=100000, learning_rate=0.0001, lambda_val=0.1)
z = np.dot(X_test, w) + b
predict_y = sigmoid(z)
binary_predictions = (predict_y >= 0.65).astype(int)

print(binary_predictions)

def confusion_matrix(true_labels, predicted_labels):
    TP = np.sum((true_labels == 1) & (predicted_labels == 1))
    TN = np.sum((true_labels == 0) & (predicted_labels == 0))
    FP = np.sum((true_labels == 0) & (predicted_labels == 1))
    FN = np.sum((true_labels == 1) & (predicted_labels == 0))
    return TP, TN, FP, FN

TP, TN, FP, FN = confusion_matrix(Y_test, binary_predictions)

accuracy = (TP + TN) / (TP + TN + FN + FP)

precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

precision_z = TN / (TN + FN) if (TN + FN) > 0 else 0.0
precision_o = TP / (TP + FP) if (TP + FP) > 0 else 0.0
rec_z = TN / (TN + FP) if (TN + FP) > 0 else 0.0
rec_o = TP / (TP + FN) if (TP + FN) > 0 else 0.0
f1_z = 2 * (precision_z * rec_z) / (precision_z + rec_z) if (precision_z + rec_z) > 0 else 0.0
f1_o = 2 * (precision_o * rec_o) / (precision_o + rec_o) if (precision_o + rec_o) > 0 else 0.0

macro_avg_precision = (precision_z + precision_o) / 2
macro_avg_recall = (rec_z + rec_o) / 2
macro_avg_f1 = (f1_z + f1_o) / 2

total_samples = TP + TN + FP + FN
weighted_avg_precision = (precision_z * (TN + FP) + precision_o * (TP + FN)) / total_samples
weighted_avg_recall = (rec_z * (TN + FP) + rec_o * (TP + FN)) / total_samples
weighted_avg_f1 = (f1_z * (TN + FP) + f1_o * (TP + FN)) / total_samples

class_rep = {
    'Metric': [
        'Precision_1', 'Recall_1', 'F1-Score_1',
        'Precision_0', 'Recall_0', 'F1-Score_0',
        'Accuracy', 'Macro Average Precision', 'Macro Average Recall',
        'Macro Average F1 Score', 'Weighted Average Precision',
        'Weighted Average Recall', 'Weighted Average F1'
    ],
    'Value': [round(x, 2) for x in [
        precision, recall, f1, precision_z, rec_z, f1_z, accuracy,
        macro_avg_precision, macro_avg_recall, macro_avg_f1,
        weighted_avg_precision, weighted_avg_recall, weighted_avg_f1
    ]]
}

class_rep_metric = pd.DataFrame(class_rep)
print("Classification Report for Logistic Regression after Regularization and Feature Scaling")
print(class_rep_metric)

"""Using the Mean squared error performed really worse in terms of accuracy. This is due to the Non convex nature of MSE which means there might be more than one local minima found when it is used. This makes the gradient descent to not converge properly leading to incorrect accuracies."""

# def forward_step():
#   selected_columns = []  # Start with an empty list of selected features
#   remaining_features = list(range(X_train.shape[1]))  # List of all feature indices
#   current_acc=-float('inf')
#   # n=X_train.shape[1]
#   # Stepwise regression
#   for i in range(X_train.shape[1]):
#       best_acc = current_acc
#       best_feature = None
#       best_w, best_b = None, None

#       # Try adding each remaining feature
#       for feature in remaining_features:
#           new_features = selected_columns + [feature]  # Add current feature to selected features
#           print(new_features)
#           model = X_train[:, new_features]  # Select the columns for the current model



#           w,b = gradient_descent(model, Y_train, gradient_dj, costfunct, iterators=10000, learning_rate=0.001, lambda_val=0.1)

#           X_subset = X_train[:, new_features]
#           model_test=X_test[:, new_features]
#           acc = evaluate_accuracy(X_subset, Y_train, w, b)
#           acc_test=evaluate_accuracy(model_test, Y_test, w, b)

#           z=np.dot(model_test, w)+b
#           predict_y=sigmoid(z)
#           binary_predictions=(predict_y>=0.65).astype(int)
#           conf_mat=confusion_matrix(Y_test, binary_predictions)
#           print(f"Confusion Matrix:\n TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}")

#           # Keep the best feature based on ACC
#           if acc > best_acc:
#               best_acc = acc
#               best_test_acc=acc_test
#               best_feature = feature
#               best_model = model
#               best_w, best_b = w,b

#       # If ACC improves, add the feature and update remaining features
#       if best_feature is not None:
#           selected_columns.append(best_feature)  # Add the best feature to selected
#           remaining_features.remove(best_feature)  # Remove the best feature from remaining
#           current_acc = best_acc
#           print(f"Iteration {i+1}: Added feature {best_feature}, ACC: {best_acc}")
#           print(f"Best test Accuaracy: {best_test_acc}")
#       else:
#           print(f"ACC value obtained: {acc} which is smaller than the best Acc obtained before so the loop stoped")
#           break


#   return selected_columns, best_acc, best_test_acc, TP, TN, FP, FN

# selected_columns, bestAcc, bestTestAcc,TP, TN, FP, FN = forward_step()
# print(len(selected_columns))
# selected_columns_new=[]
# # Final selected features
# for i in range(len(selected_columns)):
#   Best_features=cols[selected_columns[i]]
#   selected_columns_new.append(Best_features)
#   print(Best_features)
# print(f"Best accc: {bestAcc}")
# print(f"Best Test acc: {bestTestAcc}")

# def confusion_matrix(true_labels, predicted_labels):
#     TP = np.sum((true_labels == 1) & (predicted_labels == 1))
#     TN = np.sum((true_labels == 0) & (predicted_labels == 0))
#     FP = np.sum((true_labels == 0) & (predicted_labels == 1))
#     FN = np.sum((true_labels == 1) & (predicted_labels == 0))
#     return TP, TN, FP, FN

# TP, TN, FP, FN = confusion_matrix(Y_test, binary_predictions)

# # Fix accuracy formula
# accuracy = (TP + TN) / (TP + TN + FN + FP)

# # Prevent division by zero
# precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
# recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
# f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

# precision_z = TN / (TN + FN) if (TN + FN) > 0 else 0.0
# precision_o = TP / (TP + FP) if (TP + FP) > 0 else 0.0
# rec_z = TN / (TN + FP) if (TN + FP) > 0 else 0.0
# rec_o = TP / (TP + FN) if (TP + FN) > 0 else 0.0
# f1_z = 2 * (precision_z * rec_z) / (precision_z + rec_z) if (precision_z + rec_z) > 0 else 0.0
# f1_o = 2 * (precision_o * rec_o) / (precision_o + rec_o) if (precision_o + rec_o) > 0 else 0.0

# # Fix macro averages
# macro_avg_precision = (precision_z + precision_o) / 2
# macro_avg_recall = (rec_z + rec_o) / 2
# macro_avg_f1 = (f1_z + f1_o) / 2

# # Fix weighted averages
# total_samples = TP + TN + FP + FN
# weighted_avg_precision = (precision_z * (TN + FP) + precision_o * (TP + FN)) / total_samples
# weighted_avg_recall = (rec_z * (TN + FP) + rec_o * (TP + FN)) / total_samples
# weighted_avg_f1 = (f1_z * (TN + FP) + f1_o * (TP + FN)) / total_samples

# # Create classification report dataframe
# class_rep = {
#     'Metric': [
#         'Precision_1', 'Recall_1', 'F1-Score_1',
#         'Precision_0', 'Recall_0', 'F1-Score_0',
#         'Accuracy', 'Macro Average Precision', 'Macro Average Recall',
#         'Macro Average F1 Score', 'Weighted Average Precision',
#         'Weighted Average Recall', 'Weighted Average F1'
#     ],
#     'Value': [round(x, 2) for x in [
#         precision, recall, f1, precision_z, rec_z, f1_z, accuracy,
#         macro_avg_precision, macro_avg_recall, macro_avg_f1,
#         weighted_avg_precision, weighted_avg_recall, weighted_avg_f1
#     ]]
# }

# class_rep_metric = pd.DataFrame(class_rep)
# print("Classification Report for Logistic Regression after Regularization and Feature Scaling")
# print(class_rep_metric)