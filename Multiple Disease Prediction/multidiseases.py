# -*- coding: utf-8 -*-
"""MultiDiseases.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XwuSWbyGMRLGlnFuxt-DLFkQbblWPAhF

#**Importing the requires dependencies**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
import seaborn as sns
from sklearn.svm import SVC
from sklearn.metrics import f1_score, accuracy_score, confusion_matrix,classification_report,precision_score,roc_curve
from yellowbrick.classifier import ConfusionMatrix

"""#**Data Collection and Analysis**"""

ds=pd.read_csv("dataset.csv")
ds=shuffle(ds, random_state=27)
ds.head()#Print five rows after getting shuffled

#To get the rows and columns
ds.shape

"""Here we have loads of "_" so remove this
replace
"""

#To replace unwanted character
for cols in ds.columns:
  ds[cols]=ds[cols].str.replace("_"," ")
ds.head()

#To get the statistical data
ds.describe()

"""#**Cleaning**

To Check for NaN values
"""

nullval = ds.isnull().sum()
nullval=nullval.to_frame(name='count')
print(nullval)

"""**To Show the graph for nan values in the data set**"""

plt.figure(figsize=(10,5))
plt.plot(nullval.index, nullval['count'])
plt.xticks(nullval.index, nullval.index, rotation=45)
plt.title('Before removing Null values')
plt.xlabel('Column names')
plt.margins(0.1)
plt.grid(True)
plt.show()

!pip install plotly

import matplotlib.pyplot as plot

"""**Remove the trailing space from the symptom columns**"""

cols = ds.columns
data = ds[cols].values.flatten()

s = pd.Series(data)
s = s.str.strip()
s = s.values.reshape(ds.shape)

ds = pd.DataFrame(s, columns=ds.columns)
ds.head()

print(cols)

"""System Sevierity Rank"""

ds=ds.fillna(0)
ds.head()

ss=pd.read_csv("Symptom-severity.csv")
ss['Symptom'] = ss['Symptom'].str.replace('_',' ')
ss.head(10)

# prompt: Using dataframe ds: CAn you provide me with the symptoms table

symptoms_table = ds.melt(id_vars=['Disease'], var_name='Symptom', value_name='Symptom_value')
symptoms_table

"""**Get the overall list of Symptoms**"""

ds['Disease'].unique()

ss['Symptom'].unique() # ss['Symptom'].drop_duplicates()

ss['Symptom'].drop_duplicates()

"""**Symptoms** **Ranking**
This is done by comparing ds and ss (two data frames ) and if the syptomps doesn't have a rank in ss it won't assign any rank in ds so the name will appear in the data frame.
"""

vals = ds.values

symptoms = ss['Symptom'].unique()

# Create a dictionary mapping symptoms to their weights
symptomWeight = dict(zip(ss['Symptom'], ss['weight']))

# Replace symptoms in the DataFrame 'ds' with their corresponding weights
ds.replace(symptomWeight, inplace=True)

# Create a DataFrame 'd' using the modified values and original columns
d = pd.DataFrame(vals, columns=cols)

d.head(50)

"""**The Symptoms with no rank**
There are three symptoms with no rank so replace this using this following code
"""

ds = d.replace('foul smell of urine',0).replace('dischromic  patches', 0).replace('spotting  urination',0)
ds.head(50)

"""**Check whether there is any null values**"""

nullval = ds.isnull().sum()
nullval=nullval.to_frame(name="count")
print(nullval)

plt.figure(figsize=(10, 5))
plt.plot(nullval.index, nullval['count'])
plt.xticks(rotation=45)
plt.title("After removing the null values")
plt.xlabel("Column names")
plt.margins(0.1)
plt.show()

print(ds.info())

print("Number of Symptoms to identify the diseases: ", len(ss['Symptom'].drop_duplicates()))
print("Number of Diseases: ", len(ds['Disease'].drop_duplicates()))

"""**Now train and test**"""

ds.describe()

data= ds.iloc[:,1:].values
labels=ds['Disease'].values

"""#**Designing the training set and testing set(test=20% and training set=80%)**"""

from sklearn.model_selection import train_test_split,KFold,cross_val_score,GridSearchCV

train_x, test_x, train_y, test_y = train_test_split(data, labels, train_size=0.8, random_state=27)
print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)

from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier

# Define range of values for max_depth to be evaluated
max_depth_values = range(1, 21)

# Initialize an empty list to store cross-validation scores for each max_depth value
cvScores = []

# Iterate over each max_depth value
for max_depth in max_depth_values:
    # Initialize DecisionTreeClassifier with current max_depth value
    tree = DecisionTreeClassifier(max_depth=max_depth, random_state=27)

    # Perform cross-validation with 5 folds and calculate mean accuracy
    scores = cross_val_score(tree, train_x, train_y, cv=5)
    meanAccuracy = np.mean(scores)

    # Append mean accuracy to the list of cross-validation scores
    cvScores.append(meanAccuracy)

# Find the index of the max cross-validation score
bestIndex = np.argmax(cvScores)

# Get the optimal max_depth value corresponding to the best index
optimalMaxDepth = max_depth_values[bestIndex]

print("Optimal max_depth:", optimalMaxDepth)

from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'max_depth': [5, 10, 15, 19, 20]
}

# Initialize Random Forest classifier
rf_classifier = DecisionTreeClassifier(random_state=42)

# Perform grid search cross-validation
grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(train_x, train_y)

# Get best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Score:", best_score)

"""The Optimal max_depth is 19 by not using the manual method instead we used is obtained.
Now we useModels to predict and give the output them in a graph format

**DECISION TREE**
"""

import time
from sklearn.metrics import r2_score

tree =DecisionTreeClassifier(random_state=27,max_depth=19)
start_time=time.time()
tree.fit(train_x, train_y)
print(time.time()-start_time)
prediction=tree.predict(test_x)
classificationReport=classification_report(test_y, prediction, output_dict=True)
ds_report = pd.DataFrame(classificationReport).transpose()
print(ds_report)
print('\nF1-score =', f1_score(test_y, prediction, average='macro'))
print('Accuracy% =', accuracy_score(test_y, prediction)*100)

#Plot the confusson matrix
print('F1-score =', f1_score(test_y, prediction, average='macro'))
print('Accuracy% =', accuracy_score(test_y, prediction)*100)
confusionMatrix = confusion_matrix(test_y, prediction)
ds_ConfusionReport = pd.DataFrame(confusionMatrix, index=ds['Disease'].drop_duplicates(), columns=ds['Disease'].drop_duplicates())
plt.figure(figsize=(12, 10))
sns.heatmap(ds_ConfusionReport, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

kfold = KFold(n_splits=10,shuffle=True,random_state=27)
DS_train =cross_val_score(tree, train_x, train_y, cv=kfold, scoring='accuracy')
pd.DataFrame(DS_train,columns=['Scores'])
print("Mean Accuracy: %.3f%% " %(DS_train.mean()*100.0))
print("Standard Deviation: %.3f%%" %(DS_train.std()*100.0))

kfold = KFold(n_splits=10,shuffle=True,random_state=42)
DS_Test =cross_val_score(tree, test_x, test_y, cv=kfold, scoring='accuracy')
pd.DataFrame(DS_Test,columns=['Scores'])
print("Mean Accuracy: %.3f%% " %(DS_Test.mean()*100.0))
print("Standard Deviation: %.3f%%" %(DS_Test.std()*100.0))

"""**RANDOM FOREST**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'max_depth': [5, 10, 15, 19, 20]
}

# Initialize Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=27)

# Perform grid search cross-validation
grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(train_x, train_y)

# Get best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Score:", best_score)

# prompt: Train a model using random forest
import time
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier

# Create a RandomForestClassifier object
start_time= time.time()
rf = RandomForestClassifier(n_estimators=100, random_state=27)

# Train the model on the training data
rf.fit(train_x, train_y)
print(time.time()- start_time)
# Make predictions on the test data
rf_predictions = rf.predict(test_x)

# Evaluate the model's performance
rf_accuracy = accuracy_score(test_y, rf_predictions)*100
rf_f1_score = f1_score(test_y, rf_predictions, average='macro')
print(test_x[0])
print(rf_predictions[0])
# Print the results
print('Random Forest Accuracy:', rf_accuracy)
print('Random Forest F1-Score:', rf_f1_score)

#Obtain the classification report
classificationReport=classification_report(test_y, rf_predictions, output_dict=True)
ds_report = pd.DataFrame(classificationReport).transpose()
print(ds_report)

import time
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier

# Create a RandomForestClassifier object
start_time= time.time()
rf = RandomForestClassifier(n_estimators=100, random_state=27)

# Train the model on the training data
rf.fit(train_x, train_y)
print(time.time()- start_time)

# Create a confusion matrix
rf_cm = pd.DataFrame(confusionMatrix, index=ds['Disease'].drop_duplicates(), columns=ds['Disease'].drop_duplicates())

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**For Training set**"""

kfold = KFold(n_splits=10,shuffle=True,random_state=27)# Changing the splits can improve the accuracy of the mean smaller the better and its based on the problem we are dealing with
randomForestTrain =cross_val_score(rf, train_x, train_y, cv=kfold, scoring="accuracy")
pd.DataFrame(randomForestTrain,columns=["Scores"])
print("Mean Accuracy: %.3f%% "% (randomForestTrain.mean()*100.0))
print("Standard Deviation: %.3f%%" % (randomForestTrain.std()*100.0))

"""**For Test Set**"""

kfold=KFold(n_splits=10, shuffle=True, random_state=27)
randomForestTest=cross_val_score(rf, test_x, test_y, cv=kfold, scoring="accuracy")
pd.DataFrame(randomForestTest, columns=["Scores"])
print("Mean Accuracy: %.3f%%" %(randomForestTest.mean()*100.0))
print("Standard Deviation: %.3f%%" %(randomForestTest.std()*100.0))

"""**MLP**"""

import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import KFold, cross_val_score

# Create an MLPClassifier object
start_time = time.time()
mlp = MLPClassifier(hidden_layer_sizes=(500,), activation='relu', solver='adam', random_state=27)

# Train the model on the training data
mlp.fit(train_x, train_y)
print("Training time:", time.time() - start_time)
mlp_predictions = mlp.predict(test_x)

classificationReport=classification_report(test_y, mlp_predictions, output_dict=True)
ds_report = pd.DataFrame(classificationReport).transpose()
print(ds_report)

# Create a confusion matrix
mlp_cm = pd.DataFrame(confusionMatrix, index=ds['Disease'].drop_duplicates(), columns=ds['Disease'].drop_duplicates())

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(mlp_cm, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Multi-Layer Perceptron Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

import warnings

warnings.filterwarnings("ignore")

# Cross-validation
kfold = KFold(n_splits=10, shuffle=True, random_state=27)
mlp_train_scores = cross_val_score(mlp, train_x, train_y, cv=kfold, scoring="accuracy")
pd.DataFrame(mlp_train_scores, columns=["Scores"])
# Print mean accuracy and standard deviation
print("Mean Accuracy: %.3f%%" % (mlp_train_scores.mean() * 100.0))
print("Standard Deviation: %.3f%%" % (mlp_train_scores.std() * 100.0))

kfold = KFold(n_splits=10, shuffle=True, random_state=27)
mlp_test_scores = cross_val_score(mlp, test_x, test_y, cv=kfold, scoring="accuracy")
pd.DataFrame(mlp_test_scores, columns=["Scores"])
# Print mean accuracy and standard deviation
print("Mean Accuracy: %.3f%%" % (mlp_test_scores.mean() * 100.0))
print("Standard Deviation: %.3f%%" % (mlp_test_scores.std() * 100.0))

"""KNN"""

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
test_x = imputer.fit_transform(test_x)

import time
from sklearn.neighbors import KNeighborsClassifier
start_time= time.time()
KNN = KNeighborsClassifier().fit(train_x, train_y)
print(time.time()- start_time)

y_pred = KNN.predict(test_x)
print(classification_report(test_y,y_pred))

kfold = KFold(n_splits=10,shuffle=True,random_state=27)# Changing the splits can improve the accuracy of the mean smaller the better and its based on the problem we are dealing with
KnnTrain =cross_val_score(KNN, train_x, train_y, cv=kfold, scoring="accuracy")
pd.DataFrame(KnnTrain,columns=["Scores"])
print("Mean Accuracy: %.3f%% "% (KnnTrain.mean()*100.0))
print("Standard Deviation: %.3f%%" % (KnnTrain.std()*100.0))

kfold = KFold(n_splits=10,shuffle=True,random_state=27)# Changing the splits can improve the accuracy of the mean smaller the better and its based on the problem we are dealing with
KnnTest =cross_val_score(KNN, test_x, test_y, cv=kfold, scoring="accuracy")
pd.DataFrame(KnnTest,columns=["Scores"])
print("Mean Accuracy: %.3f%% "% (KnnTest.mean()*100.0))
print("Standard Deviation: %.3f%%" % (KnnTest.std()*100.0))

!pip install "dask[distributed]" --upgrade

"""#Using Dask and Joblib"""

from dask.distributed import Client, progress
client = Client(processes=False, threads_per_worker=4,
                n_workers=10, memory_limit='3GB')
client

"""**Random Forest using Dask**"""

import time
import joblib
import dask
start_time= time.time()
with joblib.parallel_backend('dask'):
  rf = RandomForestClassifier(n_estimators=100, random_state=27).fit(train_x, train_y)
print(time.time()- start_time)

y_pred = rf.predict(test_x)
print(classification_report(test_y,y_pred))

"""**MLP**"""

import time
import joblib
import dask
start_time= time.time()
with joblib.parallel_backend('dask'):
  mlp = MLPClassifier(hidden_layer_sizes=(500,), activation='relu', solver='adam', random_state=27).fit(train_x, train_y)
print(time.time()- start_time)

y_pred = mlp.predict(test_x)
print(classification_report(test_y,y_pred))

"""**Decision Tree using dask**"""

import time
import joblib
import dask
start_time= time.time()
with joblib.parallel_backend('dask'):
  t=tree.fit(train_x, train_y)
print(time.time()- start_time)

y_pred = t.predict(test_x)
print(classification_report(test_y,y_pred))

"""**KNN using dask**"""

import time
start_time= time.time()
import joblib
import dask
with joblib.parallel_backend('dask'):
    Knn = KNeighborsClassifier().fit(train_x, train_y)
print(time.time()- start_time)

y_preds = Knn.predict(test_x)
print(classification_report(test_y,y_preds))

"""#Manually testing and creating the prediction result"""

description = pd.read_csv("symptom_Description.csv")

description.head(10)

extra=pd.read_csv("symptom_precaution.csv")

extra.head(10)

doctors=pd.read_csv("symptom_doctors.csv")

doctors.head(10)

model_name = "random_forest_model.joblib"  # Example model name
directory_path = "Save"  # Example directory path

# Save the model to the specified directory
joblib.dump(rf, directory_path + model_name)

load_model=joblib.load("Saverandom_forest_model.joblib")
print(load_model)

def predd(x, *symptoms):
    # Convert symptoms to weights
    weights = ss.set_index('Symptom')['weight'].to_dict()
    symptom_weights = [weights.get(symptom, 0) for symptom in symptoms]

    # Predict disease
    pred = x.predict([symptom_weights])[0]

    # Display disease information
    disease_info = description[description['Disease'] == pred]
    disease_description = disease_info.iloc[0]['Description']
    precautions = extra[extra['Disease'] == pred].iloc[0, 1:].tolist()

    #Display doctors name and clinic details
    doc_info=description[description['Disease'] == pred]
    doc_det=doc_info.iloc[0]['Description']
    docs=doctors[doctors['Disease'] == pred].iloc[0, 1:].tolist()

    print("The Disease Name:", pred)
    print("The Disease Description:", disease_description)
    print("Recommended Things to do at home:")

    for precaution in precautions:
        print(precaution)
    print("Clinics and the Doctors: ")
    for doct in docs:
      print(doct)

"""# Comparison between algorithms

**Training Accuracy**
"""

noOfGroups = 3
algorithms = ("Multi-Layer Perceptron", "Random Forest", "KNN")
trainingAccuracy = (mlp_train_scores.mean()*100.0, randomForestTrain.mean()*100.0, KnnTrain.mean()*100.0)
print(trainingAccuracy)

"""**Test Accuracy**"""

testAccuracy = (mlp_test_scores.mean()*100.0, randomForestTest.mean()*100.0, KnnTest.mean()*100.0)
print(testAccuracy)

"""**Standard Deviation**"""

standardDeviation = (mlp_test_scores.std()*100.0, randomForestTest.std()*100.0, KnnTest.std()*100.0)
print(standardDeviation)

# create plot
fig, ax = plt.subplots(figsize=(15, 10))
index = np.arange(noOfGroups)
bar_width = 0.3
opacity = 0.8

# Plot bars for training accuracy
rects1 = plt.bar(index, trainingAccuracy, bar_width, alpha=opacity, color='blue', label='Train')

# Plot bars for test accuracy
rects2 = plt.bar(index + bar_width, testAccuracy, bar_width, alpha=opacity, color='green', label='Test')

# Plot bars for standard deviation
rects3 = plt.bar(index + bar_width, standardDeviation, bar_width, alpha=opacity, color='red', label='Standard Deviation')

plt.xlabel('Algorithms')  # x axis label
plt.ylabel('Accuracy (%)')  # y axis label
plt.ylim(0, 115)
plt.title('Algorithm Accuracies')  # plot title
plt.xticks(index + bar_width, algorithms)  # x axis data labels
plt.legend(loc='upper right')  # show legend

for rect1, rect2, rect3, train_acc, test_acc, std_dev in zip(rects1, rects2, rects3, trainingAccuracy, testAccuracy, standardDeviation):
    ax.text(rect1.get_x() + rect1.get_width() / 2, rect1.get_height() + 1, f'{round(train_acc, 2)}', ha='center', va='bottom')
    ax.text(rect2.get_x() + rect2.get_width() / 2, rect2.get_height() + 1, f'{round(test_acc, 2)}', ha='center', va='bottom')
    ax.text(rect3.get_x() + rect3.get_width() / 2, rect3.get_height() + 1, f'{round(std_dev, 2)}', ha='center', va='bottom')

plt.show()

"""#Print out the prediction for the user"""

sympList=ss["Symptom"].to_list()
predd(rf, sympList[0],sympList[60],sympList[5],sympList[70],sympList[100],sympList[6],0,0,0,0,0,0,0,0,0,0,0)