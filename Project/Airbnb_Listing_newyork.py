# -*- coding: utf-8 -*-
"""project_ashik.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OqPNT1a0qYafltexoQ6oOv8FcAcppxGZ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from IPython.display import display

!pip install prophet
from prophet import Prophet

df= pd.read_csv("/content/AB_NYC_2019.csv")

df.head(10)

df = df.sample(frac=1, random_state=42)

# Drop outliers in price and zero-price listings
df = df[(df['price'] > 0) & (df['price'] < 1000)]

df['room_type'].unique()

df['neighbourhood_group'].unique()

#Categorial
df['room_type_encoded'] = df['room_type'].map({'Entire home/apt': 0, 'Private room': 1, 'Shared room': 2})
df['neighbourhood_group_encoded'] = df['neighbourhood_group'].map({
    'Manhattan': 0, 'Brooklyn': 1, 'Queens': 2, 'Bronx': 3, 'Staten Island': 4
})

df.isnull().sum()

#unwanted feature removal
df.drop(['id', 'name', 'host_name', 'host_id'], axis=1, inplace=True)

df['reviews_per_month'] = df['reviews_per_month'].fillna(0)

high_demand=[]
for avail in df['availability_365']:
  if avail<100:
    high_demand.append(1)
  else:
    high_demand.append(0)
df['high_demand']=high_demand

print(df['high_demand'])

df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')
df['last_review_year'] = df['last_review'].dt.year
df['last_review_month'] = df['last_review'].dt.month

df['last_review_month']=df['last_review_month'].fillna(0)
df['last_review_year']=df['last_review_year'].fillna(df['last_review_year'].median())

df['last_review_month']

df['last_review_year']

# One-hot encoding
one_hot_cols = ['room_type', 'neighbourhood_group']

df[one_hot_cols]

df_encoded = pd.get_dummies(df, columns=one_hot_cols, drop_first=True)

df_encoded.head()

features_dumm=[]
for cols in df_encoded.columns:
  if cols.startswith('room_type_') or cols.startswith('neighbourhood_group'):
    features_dumm.append(cols)

selected_features=selected_features = [
    'price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month',
    'calculated_host_listings_count', 'availability_365', 'latitude', 'longitude',
    'last_review_year', 'last_review_month', 'high_demand'
] + features_dumm

df_model = df_encoded[selected_features]

print("\n\tEnhanced dataset: ")
df_model.head()

df_model.isnull().sum()

print("\nSummary statistics:")
df_model.describe(include='all')

display(df_model)

from sklearn.model_selection import train_test_split

X=df_model.drop(['price', 'high_demand'], axis=1)
y_price=df_model['price']
y_demand=df_model['high_demand']

X.columns

# Split into train+val and test (80%-20%)
X_temp, X_test, y_price_temp, y_price_test, y_demand_temp, y_demand_test = train_test_split(
    X, y_price, y_demand, test_size=0.2, random_state=42)

# Then split train+val into train and validation (60%-20%)
X_train, X_val, y_price_train, y_price_val, y_demand_train, y_demand_val = train_test_split(
    X_temp, y_price_temp, y_demand_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2

print("Train size:", X_train.shape)
print("Validation size:", X_val.shape)
print("Test size:", X_test.shape)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_price_train)

# Predict and evaluate
y_price_pred_val = lin_reg.predict(X_val)
y_price_pred_test = lin_reg.predict(X_test)
rmse_val = np.sqrt(mean_squared_error(y_price_val, y_price_pred_val))
r2_val = r2_score(y_price_val, y_price_pred_val)
rmse_test = np.sqrt(mean_squared_error(y_price_test, y_price_pred_test))
r2_test = r2_score(y_price_test, y_price_pred_test)

print("\n\tLinear Regression Validation Metrics:")
print("RMSE:", round(rmse_val,4))
print("R2 Score:", round(r2_val, 4))

print("\n\tLinear Regression Test Metrics:")
print("RMSE:", round(rmse_test,4))
print("R2 Score:", round(r2_test, 4))

from sklearn.ensemble import RandomForestRegressor

best_rmse = float('inf')
best_model = None
best_n = 0

# Try different numbers of trees
for n in [50, 100, 200, 300]:
    rf = RandomForestRegressor(n_estimators=n, random_state=42)
    rf.fit(X_train, y_price_train)
    y_val_pred = rf.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_price_val, y_val_pred))
    print(f"n_estimators = {n} → Validation RMSE = {rmse:.2f}")

    if rmse < best_rmse:
        best_rmse = rmse
        best_model = rf
        best_n = n

# Best model selected
print(f"\nBest model: {best_n} trees with Validation RMSE = {best_rmse:.2f}")

# Evaluate best model on test set
y_rf_val_pred = best_model.predict(X_val)
y_rf_test_pred = best_model.predict(X_test)

rf_rmse_val = np.sqrt(mean_squared_error(y_price_val, y_rf_val_pred))
rf_r2_val = r2_score(y_price_val, y_rf_val_pred)

rf_rmse_test = np.sqrt(mean_squared_error(y_price_test, y_rf_test_pred))
rf_r2_test = r2_score(y_price_test, y_rf_test_pred)

print(f"\nFinal Test RMSE = {rf_rmse_test:.2f}, R² = {rf_r2_test:.4f}")

print("\n\tRandom Forest Validation Metrics:")
print("RMSE:", round(rf_rmse_val,4))
print("R2 Score:", round(rf_r2_val, 4))

print("\n\tRandom Forest Test Metrics:")
print("RMSE:", round(rf_rmse_test,4))
print("R2 Score:", round(rf_r2_test, 4))

# Comparison table
comparison_metrics = {
    "Model": ["Linear Regression", "Random Forest"],
    "Validation RMSE": [round(rmse_val, 2), round(rf_rmse_val, 2)],
    "Validation R²": [round(r2_val, 4), round(rf_r2_val, 4)],
    "Test RMSE": [round(rmse_test, 2), round(rf_rmse_test, 2)],
    "Test R²": [round(r2_test, 4), round(rf_r2_test, 4)]
}

comparison_df = pd.DataFrame(comparison_metrics)

comparison_df

feature_importances = best_model.feature_importances_
feature_names = X_train.columns

importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(12, 8))
plt.barh(importance_df['Feature'][:10][::-1], importance_df['Importance'][:10][::-1])
plt.xlabel("Feature Importance")
plt.title("Top 10 Important Features for Price Prediction (Random Forest)")
plt.tight_layout()
plt.show()

# Plot predicted vs actual prices on the test set
plt.figure(figsize=(8, 6))
plt.scatter(y_price_test, y_rf_test_pred, alpha=0.5)
plt.plot([y_price_test.min(), y_price_test.max()], [y_price_test.min(), y_price_test.max()],'r--', lw=2)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Predicted vs Actual Airbnb Prices (Random Forest)")
plt.grid(True)
plt.tight_layout()
plt.show()

"""Classification Model"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay

best_acc = 0
best_model = None
best_n = 0

# Hyperparameter tuning for number of trees
for n in [50, 100, 200, 300]:
    model = RandomForestClassifier(n_estimators=n, random_state=42)
    model.fit(X_train, y_demand_train)
    y_val_pred = model.predict(X_val)
    acc = accuracy_score(y_demand_val, y_val_pred)
    print(f"n_estimators = {n} → Validation Accuracy = {acc:.4f}")

    if acc > best_acc:
        best_acc = acc
        best_model = model
        best_n = n

print(f"\nBest model: n_estimators = {best_n}, Validation Accuracy = {best_acc:.4f}")

# Final predictions from best model
y_val_pred_hd = best_model.predict(X_val)
y_test_pred_hd = best_model.predict(X_test)

# Classification reports
print("\n\tClassification Report (Validation Set):")
print(classification_report(y_demand_val, y_val_pred_hd))

print("\n\tClassification Report (Test Set):")
print(classification_report(y_demand_test, y_test_pred_hd))

# Confusion matrix visualization
ConfusionMatrixDisplay.from_predictions(y_demand_test, y_test_pred_hd)
plt.title("\tConfusion Matrix (Test Set - High Demand: Classification)")
plt.tight_layout()
plt.show()

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg.fit(X_train, y_demand_train)

y_val_pred_log = log_reg.predict(X_val)
y_test_pred_log = log_reg.predict(X_test)

from sklearn.metrics import ConfusionMatrixDisplay

print("Classification Report (Validation - Logistic Regression):")
print(classification_report(y_demand_val, y_val_pred_log))

print("\nClassification Report (Test - Logistic Regression):")
print(classification_report(y_demand_test, y_test_pred_log))

# Confusion Matrix for test
ConfusionMatrixDisplay.from_predictions(y_demand_test, y_test_pred_log, cmap="Purples")
plt.title("Confusion Matrix - Logistic Regression (Test Set)")
plt.tight_layout()
plt.show()

"""Prophet used for forecast price trends

"""

df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')
df = df.dropna(subset=['last_review', 'price'])

df = df[df['price'] < 1000]

monthly_prices = df.groupby(df['last_review'].dt.to_period('M'))['price'].mean().reset_index()
monthly_prices['last_review'] = monthly_prices['last_review'].dt.to_timestamp()
monthly_prices.columns = ['ds', 'y']

model = Prophet()
model.fit(monthly_prices)

future = model.make_future_dataframe(periods=6, freq='M')
forecast = model.predict(future)

fig1 = model.plot(forecast)
plt.title("Prophet Forecast of Monthly Airbnb Prices")
plt.xlabel("Date")
plt.ylabel("Average Price")
plt.grid(True)
plt.show()


fig2 = model.plot_components(forecast)
plt.show()

monthly_prices.head()

forecast.head()

"""(Optional) follium"""

import folium
from folium.plugins import HeatMap

# Sample for performance
sample_df = df_model.sample(5000, random_state=42)

#Listings distribution
center_lat = sample_df['latitude'].mean()
center_lon = sample_df['longitude'].mean()
map_listings = folium.Map(location=[center_lat, center_lon], zoom_start=11)

for idx, row in sample_df.iterrows():
    folium.CircleMarker(location=[row['latitude'], row['longitude']], radius=1, color='blue', fill=True, fill_opacity=0.5).add_to(map_listings)

#Price heatmap
price_df = sample_df[sample_df['price'] < 500]
heat_data = [[row['latitude'], row['longitude'], row['price']] for _, row in price_df.iterrows()]
map_price = folium.Map(location=[center_lat, center_lon], zoom_start=11)
HeatMap(heat_data, radius=10, blur=15, max_zoom=1).add_to(map_price)

df['high_demand']

# High Demand vs Low Demand
map_demand = folium.Map(location=[center_lat, center_lon], zoom_start=11)
for _, row in sample_df.iterrows():
    color = 'red' if row['high_demand'] == 1 else 'green'
    folium.CircleMarker(location=[row['latitude'], row['longitude']], radius=1.5, color=color, fill=True, fill_opacity=0.5).add_to(map_demand)

map_listings.save("listing_distribution_map.html")
map_price.save("price_heatmap_map.html")
map_demand.save("high_demand_map.html")