# -*- coding: utf-8 -*-
"""Question4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uGcRZzFhNvXOaKkCsI48xAMxJnaN6A4P

Regularization is a technique used in machine learning and statistics to prevent overfitting by adding a penalty to the complexity of the model. we can do it by using lamba when calculating ols here in backward stepwise regression.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv('Cancer_dataset.csv')

df.head(10)

column_for_front=['mean_radius',
'mean_perimeter',
'worst_symmetry',
'lymph_node_status',
'mean_texture',]

df[column_for_front]=df[column_for_front].fillna(0)
print(df[column_for_front].head(10))
Y=df[['tumor_size']].fillna(0)
print(Y)

X=df[column_for_front].to_numpy()
print(f"X values: \n{X[:10]}")
Y=np.array(Y)
print(f"Y values: \n{Y[:10]}")

length=len(X)
index=np.arange(length)
np.random.seed(42)
np.random.shuffle(index)
split=int(length*0.80)
X_train=X[index[:split]]
X_test=X[index[split:]]
print(f"X_Train: \n{X_train[:10]}")
print(f"X_Test: \n{X_test[:10]}")
Y_train=Y[index[:split]]
Y_test=Y[index[split:]]
print(f"Y_Train: \n{Y_train[:10]}")
print(f"Y_Test: \n{Y_test[:10]}")

#Function for BIC

def calculate_BIC(n, k, y, res_ss):
  L=(-n/2)*np.log(2*np.pi)-(n/2)*np.log(res_ss/n)-(res_ss/(2*np.var(y)))
  BIC=np.log(n)*k-2*L
  return BIC

n = len(Y_train)  # Number of training samples
k = 1  # Only the intercept is used
SSE_bias = np.sum((Y_train - Y_train.mean()) ** 2)  # Sum of Squares for bias model

bias_Bic = calculate_BIC(n, k, Y_train ,SSE_bias)  # Compute BIC
print(f"BIC for Bias-Only Model: {bias_Bic}")

#function to fit ols model
def ols(x, y, lam):
    # Add a column of ones to X for the intercept term
    X_ = np.c_[np.ones(len(x)), x]

    I = np.eye(X_.shape[1])  # Identity matrix of size
    I[0, 0] = 0

    # Compute the Ridge regression coefficients using np.dot
    inner_X = np.dot(X_.T, X_)  # X^T * X

    inv_X = np.linalg.inv(inner_X + lam * I )  # Compute inverse

    beta = np.dot(inv_X, np.dot(X_.T, y))  # (X^T X + Î»I)^-1 X^T y

    return beta

def forward_step_reg(lamb):
  selected_columns = []  # Start with an empty list of selected features
  remaining_features = list(range(X_train.shape[1]))  # List of all feature indices
  current_bic = bias_Bic  # Start with the bias-only model's BIC

  # Stepwise regression
  for i in range(5):  # upto to 5 iterations
      best_bic = current_bic
      best_feature = None
      best_model = None
      best_beta = None

      # Try adding each remaining feature
      for feature in remaining_features:
          new_features = selected_columns + [feature]  # Add current feature to selected features
          model = X_train[:, new_features]  # Select the columns for the current model
          beta = ols(model, Y_train, lamb)  # Fit the OLS model

          # Make predictions and calculate residual sum of squares
          y_pred = np.c_[np.ones(model.shape[0]), model] @ beta
          rss = np.sum((Y_train - y_pred) ** 2)  # Residual sum of squares

          # Calculate BIC for this model
          bic = calculate_BIC(n, len(new_features) + 1,Y_train, rss)  # Use the number of features
          # Keep the best feature based on BIC
          if bic < best_bic:
              best_bic = bic
              best_feature = feature
              best_model = model
              best_beta = beta

      # If BIC improves add the feature and update remaining features
      if best_feature is not None:
          selected_columns.append(best_feature)  # Add the best feature to selected
          remaining_features.remove(best_feature)  # Remove the best feature from remaining
          current_bic = best_bic  # Update the current BIC
          print(f"Iteration {i+1}: Added feature {best_feature}, BIC: {best_bic}")
      else:
          print(f"Bic value obtained: {bic} which is larger than the best bic obtained before so the loop stoped")
          break
  return selected_columns, best_bic

lamb=0.001
selected_columns, bestBic = forward_step_reg(lamb)
print(len(selected_columns))
# Final selected features
for i in range(len(selected_columns)):
  Best_features=column_for_front[selected_columns[i]]
  print(Best_features)
print(f"Best bic: {bestBic}")

length=len(X)
index=np.arange(length)
np.random.seed(42)
np.random.shuffle(index)
split=int(length*0.8)
X_train=X[index[:split]]
X_test=X[index[split:]]
print(f"X_Train: \n{X_train[:10]}")
print(f"X_Test: \n{X_test[:10]}")

for i in range(X.shape[1]):
  mean = np.mean(X_train[:, i])
  std = np.std(X_train[:, i])
  X_train[:, i] = (X_train[:, i] - mean) / std
  X_test[:, i] = (X_test[:, i] - mean) / std
  #axis=0 is used here to prevent th arning abut calculating std along the column
print(f"Standardize form of X_train: \n{X_train[0:10]}")
print(f"Standardize form of X_test: \n{X_test[0:10]}")

Y_train=Y[index[:split]]
Y_test=Y[index[split:]]
print(f"Y_Train: \n{Y_train[:10]}")
print(f"Y_Test: \n{Y_test[:10]}")

mu_Y = np.mean(Y_train)
sigma_Y = np.std(Y_train)
Y_train=(Y_train-mu_Y)/sigma_Y
Y_test=(Y_test-mu_Y)/sigma_Y

print(f"Standardize form of Y_train: \n{Y_train[0:10]}")

print(f"Standardize form of Y_test: \n{Y_test[0:10]}")

n = len(Y_train)  # Number of training samples
k_bias = 1  # Only the intercept (bias term) is used
SSE_bias = np.sum((Y_train - Y_train.mean()) ** 2)  # Residual Sum of Squares (RSS) for bias model

bias_Bic = calculate_BIC(n, k_bias, Y_train, SSE_bias)  # Compute BIC
print(f"BIC for Bias-Only Model: {bias_Bic}")

#function to fit ols model
def ols(x, y):
    # Add a column of ones to X for the intercept term (beta0)
    X_ = np.c_[np.ones(len(x)), x]

    # Compute the pseudo-inverse of X^T * X
    x_inv = np.linalg.pinv(X_.T @ X_)

    # Compute the regression coefficients (beta)
    beta = x_inv @ (X_.T) @ y

    return beta

def forward_step():
  # Assuming that the columns_to_select and other variables are already defined
  selected_columns = []  # Start with an empty list of selected features
  remaining_features = list(range(X_train.shape[1]))  # List of all feature indices
  current_bic = bias_Bic  # Start with the bias-only model's BIC

  # Stepwise regression
  for i in range(5):  # upto to 5 iterations
      best_bic = current_bic
      best_feature = None
      best_model = None
      best_beta = None

      # Try adding each remaining feature
      for feature in remaining_features:
          new_features = selected_columns + [feature]  # Add current feature to selected features
          model = X_train[:, new_features]  # Select the columns for the current model
          beta = ols(model, Y_train)  # Fit the OLS model

          # Make predictions and calculate residual sum of squares (RSS)
          y_pred = np.c_[np.ones(model.shape[0]), model] @ beta
          rss = np.sum((Y_train - y_pred) ** 2)  # Residual sum of squares

          # Calculate BIC for this model
          bic = calculate_BIC(n, len(new_features) + 1, Y_train, rss)  # Use the number of features
          # Keep the best feature based on BIC
          if bic < best_bic:
              best_bic = bic
              best_feature = feature
              best_model = model
              best_beta = beta

      # If BIC improves, add the feature and update remaining features
      if best_feature is not None:
          selected_columns.append(best_feature)  # Add the best feature to selected
          remaining_features.remove(best_feature)  # Remove the best feature from remaining
          current_bic = best_bic  # Update the current BIC
          print(f"Iteration {i+1}: Added feature {best_feature}, BIC: {best_bic}")
      else:
          print(f"Bic value obtained: {bic} which is larger than the best bic obtained before so the loop stoped")
          break
  return selected_columns, best_beta

selected_columns, best_beta = forward_step()
print(len(selected_columns))
# Final selected features
for i in range(len(selected_columns)):
  Best_features=column_for_front[selected_columns[i]]
  print(Best_features)
